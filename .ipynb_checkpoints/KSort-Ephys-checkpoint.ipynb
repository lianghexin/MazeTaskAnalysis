{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6ec071c-a651-4aca-ba7a-8ba96f81ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeinterface as si\n",
    "from spikeinterface.extractors import read_openephys\n",
    "import spikeinterface.preprocessing as spre\n",
    "import spikeinterface.sorters as ss\n",
    "import spikeinterface.postprocessing as spost\n",
    "import spikeinterface.qualitymetrics as sqm\n",
    "import spikeinterface.comparison as sc\n",
    "import spikeinterface.exporters as sexp\n",
    "import spikeinterface.widgets as sw\n",
    "\n",
    "from kilosort import io\n",
    "from kilosort import run_kilosort\n",
    "import torch\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import subprocess\n",
    "import sys\n",
    "from matplotlib import gridspec\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc07afbd-b764-4682-b3d7-c6ae30d56587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ks_files (DATA_DIRECTORY):\n",
    "    \n",
    "    recording = read_openephys(DATA_DIRECTORY)\n",
    "    probe_path = 'D:/Kilosort-main/poly3_64.prb'\n",
    "    probe = io.load_probe(probe_path)\n",
    "    dtype = np.int16\n",
    "\n",
    "    filename = DATA_DIRECTORY / 'data.bin'\n",
    "\n",
    "    print(filename)\n",
    "    print(filename.is_file())\n",
    "\n",
    "    if filename.is_file():\n",
    "        print('FILE EXISTED')\n",
    "    else:\n",
    "        filename, N, c, s, fs, probe_path = io.spikeinterface_to_binary(\n",
    "            recording, DATA_DIRECTORY, data_name='data.bin', dtype=dtype,\n",
    "            chunksize=60000, export_probe=True, probe_name=probe_path\n",
    "            )        \n",
    "\n",
    "    ks_dir = DATA_DIRECTORY / 'kilosort4'\n",
    "    if ks_dir.is_dir():\n",
    "        print('SORTED')\n",
    "    else:\n",
    "        settings = {'n_chan_bin': 64, 'dmin': 25, 'dminx': 22,'nearest_templates':64}\n",
    "        \n",
    "        ops, st, clu, tF, Wall, similar_templates, is_ref, est_contam_rate = run_kilosort(\n",
    "            settings=settings, probe=probe, filename=filename, data_dtype=dtype\n",
    "            )\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5560cbfc-ff32-426f-9c72-e5a3e7f8bff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\HexinData\\Dylan_2024-03-18_13-29-27_HPC\\data.bin\n",
      "True\n",
      "FILE EXISTED\n",
      "SORTED\n",
      "D:\\HexinData\\Dylan_2024-03-19_14-42-22_HPC\\data.bin\n",
      "True\n",
      "FILE EXISTED\n",
      "SORTED\n",
      "D:\\HexinData\\Dylan_2024-03-20_13-55-20_HPC\\data.bin\n",
      "True\n",
      "FILE EXISTED\n",
      "Using GPU for PyTorch computations. Specify `device` to change this.\n",
      "Preprocessing filters computed in  4.05s; total  4.06s\n",
      "\n",
      "computing drift\n",
      "Re-computing universal templates from data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3702/3702 [25:33<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drift computed in  1547.05s; total  1551.12s\n",
      "\n",
      "Extracting spikes using templates\n",
      "Re-computing universal templates from data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3702/3702 [25:19<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3009215 spikes extracted in  1523.38s; total  3074.53s\n",
      "\n",
      "First clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [03:00<00:00, 16.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 clusters found, in  180.45s; total  3254.97s\n",
      "\n",
      "Extracting spikes using cluster waveforms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3702/3702 [02:31<00:00, 24.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "836852 spikes extracted in  152.11s; total  3407.08s\n",
      "\n",
      "Final clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:53<00:00,  4.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 clusters found, in  53.52s; total  3460.62s\n",
      "\n",
      "Merging clusters\n",
      "31 units found, in  0.11s; total  3460.73s\n",
      "\n",
      "Saving to phy and computing refractory periods\n",
      "8 units found with good refractory periods\n",
      "\n",
      "Total runtime: 3463.04s = 00:57:43 h:m:s\n",
      "D:\\HexinData\\Dylan_2024-03-21_14-55-03_HPC\\data.bin\n",
      "False\n",
      "========================================\n",
      "Loading recording with SpikeInterface...\n",
      "number of samples: 206444910\n",
      "number of channels: 64\n",
      "numbef of segments: 1\n",
      "sampling rate: 30000.0\n",
      "dtype: int16\n",
      "========================================\n",
      "Converting 3441 data chunks with a chunksize of 60000 samples...\n",
      "24 of 3441 chunks converted...\n",
      "43 of 3441 chunks converted...\n",
      "54 of 3441 chunks converted...\n",
      "67 of 3441 chunks converted...\n",
      "82 of 3441 chunks converted...\n",
      "93 of 3441 chunks converted...\n",
      "111 of 3441 chunks converted...\n",
      "124 of 3441 chunks converted...\n",
      "132 of 3441 chunks converted...\n",
      "150 of 3441 chunks converted...\n",
      "162 of 3441 chunks converted...\n",
      "177 of 3441 chunks converted...\n",
      "189 of 3441 chunks converted...\n",
      "201 of 3441 chunks converted...\n",
      "217 of 3441 chunks converted...\n",
      "231 of 3441 chunks converted...\n",
      "243 of 3441 chunks converted...\n",
      "261 of 3441 chunks converted...\n",
      "274 of 3441 chunks converted...\n",
      "289 of 3441 chunks converted...\n",
      "300 of 3441 chunks converted...\n",
      "313 of 3441 chunks converted...\n",
      "333 of 3441 chunks converted...\n",
      "342 of 3441 chunks converted...\n",
      "353 of 3441 chunks converted...\n",
      "365 of 3441 chunks converted...\n",
      "386 of 3441 chunks converted...\n",
      "401 of 3441 chunks converted...\n",
      "409 of 3441 chunks converted...\n",
      "428 of 3441 chunks converted...\n",
      "442 of 3441 chunks converted...\n",
      "451 of 3441 chunks converted...\n",
      "467 of 3441 chunks converted...\n",
      "481 of 3441 chunks converted...\n",
      "494 of 3441 chunks converted...\n",
      "507 of 3441 chunks converted...\n",
      "520 of 3441 chunks converted...\n",
      "540 of 3441 chunks converted...\n",
      "550 of 3441 chunks converted...\n",
      "561 of 3441 chunks converted...\n",
      "576 of 3441 chunks converted...\n",
      "589 of 3441 chunks converted...\n",
      "605 of 3441 chunks converted...\n",
      "613 of 3441 chunks converted...\n",
      "627 of 3441 chunks converted...\n",
      "639 of 3441 chunks converted...\n",
      "656 of 3441 chunks converted...\n",
      "668 of 3441 chunks converted...\n",
      "686 of 3441 chunks converted...\n",
      "697 of 3441 chunks converted...\n",
      "713 of 3441 chunks converted...\n",
      "728 of 3441 chunks converted...\n",
      "738 of 3441 chunks converted...\n",
      "752 of 3441 chunks converted...\n",
      "763 of 3441 chunks converted...\n",
      "779 of 3441 chunks converted...\n",
      "795 of 3441 chunks converted...\n",
      "808 of 3441 chunks converted...\n",
      "822 of 3441 chunks converted...\n",
      "837 of 3441 chunks converted...\n",
      "849 of 3441 chunks converted...\n",
      "864 of 3441 chunks converted...\n",
      "874 of 3441 chunks converted...\n",
      "886 of 3441 chunks converted...\n",
      "902 of 3441 chunks converted...\n",
      "913 of 3441 chunks converted...\n",
      "927 of 3441 chunks converted...\n",
      "940 of 3441 chunks converted...\n",
      "960 of 3441 chunks converted...\n",
      "971 of 3441 chunks converted...\n",
      "990 of 3441 chunks converted...\n",
      "1001 of 3441 chunks converted...\n",
      "1015 of 3441 chunks converted...\n",
      "1029 of 3441 chunks converted...\n",
      "1041 of 3441 chunks converted...\n",
      "1055 of 3441 chunks converted...\n",
      "1068 of 3441 chunks converted...\n",
      "1086 of 3441 chunks converted...\n",
      "1100 of 3441 chunks converted...\n",
      "1117 of 3441 chunks converted...\n",
      "1128 of 3441 chunks converted...\n",
      "1139 of 3441 chunks converted...\n",
      "1153 of 3441 chunks converted...\n",
      "1169 of 3441 chunks converted...\n",
      "1180 of 3441 chunks converted...\n",
      "1197 of 3441 chunks converted...\n",
      "1209 of 3441 chunks converted...\n",
      "1221 of 3441 chunks converted...\n",
      "1236 of 3441 chunks converted...\n",
      "1248 of 3441 chunks converted...\n",
      "1266 of 3441 chunks converted...\n",
      "1276 of 3441 chunks converted...\n",
      "1289 of 3441 chunks converted...\n",
      "1304 of 3441 chunks converted...\n",
      "1316 of 3441 chunks converted...\n",
      "1332 of 3441 chunks converted...\n",
      "1344 of 3441 chunks converted...\n",
      "1355 of 3441 chunks converted...\n",
      "1368 of 3441 chunks converted...\n",
      "1383 of 3441 chunks converted...\n",
      "1395 of 3441 chunks converted...\n",
      "1407 of 3441 chunks converted...\n",
      "1419 of 3441 chunks converted...\n",
      "1433 of 3441 chunks converted...\n",
      "1443 of 3441 chunks converted...\n",
      "1460 of 3441 chunks converted...\n",
      "1475 of 3441 chunks converted...\n",
      "1486 of 3441 chunks converted...\n",
      "1502 of 3441 chunks converted...\n",
      "1510 of 3441 chunks converted...\n",
      "1524 of 3441 chunks converted...\n",
      "1541 of 3441 chunks converted...\n",
      "1551 of 3441 chunks converted...\n",
      "1566 of 3441 chunks converted...\n",
      "1583 of 3441 chunks converted...\n",
      "1592 of 3441 chunks converted...\n",
      "1608 of 3441 chunks converted...\n",
      "1617 of 3441 chunks converted...\n",
      "1637 of 3441 chunks converted...\n",
      "1645 of 3441 chunks converted...\n",
      "1658 of 3441 chunks converted...\n",
      "1672 of 3441 chunks converted...\n",
      "1683 of 3441 chunks converted...\n",
      "1699 of 3441 chunks converted...\n",
      "1713 of 3441 chunks converted...\n",
      "1724 of 3441 chunks converted...\n",
      "1738 of 3441 chunks converted...\n",
      "1754 of 3441 chunks converted...\n",
      "1765 of 3441 chunks converted...\n",
      "1781 of 3441 chunks converted...\n",
      "1793 of 3441 chunks converted...\n",
      "1811 of 3441 chunks converted...\n",
      "1821 of 3441 chunks converted...\n",
      "1838 of 3441 chunks converted...\n",
      "1849 of 3441 chunks converted...\n",
      "1869 of 3441 chunks converted...\n",
      "1878 of 3441 chunks converted...\n",
      "1897 of 3441 chunks converted...\n",
      "1908 of 3441 chunks converted...\n",
      "1927 of 3441 chunks converted...\n",
      "1937 of 3441 chunks converted...\n",
      "1950 of 3441 chunks converted...\n",
      "1967 of 3441 chunks converted...\n",
      "1979 of 3441 chunks converted...\n",
      "1993 of 3441 chunks converted...\n",
      "2009 of 3441 chunks converted...\n",
      "2024 of 3441 chunks converted...\n",
      "2037 of 3441 chunks converted...\n",
      "2048 of 3441 chunks converted...\n",
      "2063 of 3441 chunks converted...\n",
      "2078 of 3441 chunks converted...\n",
      "2091 of 3441 chunks converted...\n",
      "2108 of 3441 chunks converted...\n",
      "2122 of 3441 chunks converted...\n",
      "2140 of 3441 chunks converted...\n",
      "2151 of 3441 chunks converted...\n",
      "2164 of 3441 chunks converted...\n",
      "2178 of 3441 chunks converted...\n",
      "2191 of 3441 chunks converted...\n",
      "2209 of 3441 chunks converted...\n",
      "2220 of 3441 chunks converted...\n",
      "2233 of 3441 chunks converted...\n",
      "2251 of 3441 chunks converted...\n",
      "2263 of 3441 chunks converted...\n",
      "2274 of 3441 chunks converted...\n",
      "2296 of 3441 chunks converted...\n",
      "2312 of 3441 chunks converted...\n",
      "2323 of 3441 chunks converted...\n",
      "2336 of 3441 chunks converted...\n",
      "2352 of 3441 chunks converted...\n",
      "2364 of 3441 chunks converted...\n",
      "2378 of 3441 chunks converted...\n",
      "2390 of 3441 chunks converted...\n",
      "2407 of 3441 chunks converted...\n",
      "2416 of 3441 chunks converted...\n",
      "2434 of 3441 chunks converted...\n",
      "2455 of 3441 chunks converted...\n",
      "2468 of 3441 chunks converted...\n",
      "2481 of 3441 chunks converted...\n",
      "2496 of 3441 chunks converted...\n",
      "2506 of 3441 chunks converted...\n",
      "2527 of 3441 chunks converted...\n",
      "2537 of 3441 chunks converted...\n",
      "2550 of 3441 chunks converted...\n",
      "2565 of 3441 chunks converted...\n",
      "2576 of 3441 chunks converted...\n",
      "2595 of 3441 chunks converted...\n",
      "2606 of 3441 chunks converted...\n",
      "2620 of 3441 chunks converted...\n",
      "2636 of 3441 chunks converted...\n",
      "2647 of 3441 chunks converted...\n",
      "2659 of 3441 chunks converted...\n",
      "2682 of 3441 chunks converted...\n",
      "2692 of 3441 chunks converted...\n",
      "2715 of 3441 chunks converted...\n",
      "2728 of 3441 chunks converted...\n",
      "2744 of 3441 chunks converted...\n",
      "2759 of 3441 chunks converted...\n",
      "2769 of 3441 chunks converted...\n",
      "2785 of 3441 chunks converted...\n",
      "2795 of 3441 chunks converted...\n",
      "2812 of 3441 chunks converted...\n",
      "2823 of 3441 chunks converted...\n",
      "2838 of 3441 chunks converted...\n",
      "2851 of 3441 chunks converted...\n",
      "2863 of 3441 chunks converted...\n",
      "2880 of 3441 chunks converted...\n",
      "2890 of 3441 chunks converted...\n",
      "2912 of 3441 chunks converted...\n",
      "2925 of 3441 chunks converted...\n",
      "2939 of 3441 chunks converted...\n",
      "2951 of 3441 chunks converted...\n",
      "2970 of 3441 chunks converted...\n",
      "2980 of 3441 chunks converted...\n",
      "2996 of 3441 chunks converted...\n",
      "3009 of 3441 chunks converted...\n",
      "3024 of 3441 chunks converted...\n",
      "3039 of 3441 chunks converted...\n",
      "3051 of 3441 chunks converted...\n",
      "3067 of 3441 chunks converted...\n",
      "3080 of 3441 chunks converted...\n",
      "3092 of 3441 chunks converted...\n",
      "3104 of 3441 chunks converted...\n",
      "3118 of 3441 chunks converted...\n",
      "3137 of 3441 chunks converted...\n",
      "3158 of 3441 chunks converted...\n",
      "3174 of 3441 chunks converted...\n",
      "3185 of 3441 chunks converted...\n",
      "3196 of 3441 chunks converted...\n",
      "3211 of 3441 chunks converted...\n",
      "3224 of 3441 chunks converted...\n",
      "3239 of 3441 chunks converted...\n",
      "3249 of 3441 chunks converted...\n",
      "3269 of 3441 chunks converted...\n",
      "3276 of 3441 chunks converted...\n",
      "3293 of 3441 chunks converted...\n",
      "3305 of 3441 chunks converted...\n",
      "3320 of 3441 chunks converted...\n",
      "3334 of 3441 chunks converted...\n",
      "3348 of 3441 chunks converted...\n",
      "3361 of 3441 chunks converted...\n",
      "3380 of 3441 chunks converted...\n",
      "3387 of 3441 chunks converted...\n",
      "3404 of 3441 chunks converted...\n",
      "3422 of 3441 chunks converted...\n",
      "3430 of 3441 chunks converted...\n",
      "3441 of 3441 chunks converted...\n",
      "Data conversion finished.\n",
      "========================================\n",
      "SpikeInterface recording contains no probe information,\n",
      "could not write .prb file.\n",
      "Using GPU for PyTorch computations. Specify `device` to change this.\n",
      "Preprocessing filters computed in  2.75s; total  2.75s\n",
      "\n",
      "computing drift\n",
      "Re-computing universal templates from data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3441/3441 [23:01<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drift computed in  1388.08s; total  1390.83s\n",
      "\n",
      "Extracting spikes using templates\n",
      "Re-computing universal templates from data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3441/3441 [23:24<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566083 spikes extracted in  1408.33s; total  2809.25s\n",
      "\n",
      "First clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:44<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 clusters found, in  44.07s; total  2853.33s\n",
      "\n",
      "Extracting spikes using cluster waveforms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3441/3441 [02:36<00:00, 21.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589797 spikes extracted in  156.65s; total  3009.98s\n",
      "\n",
      "Final clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:43<00:00,  3.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 clusters found, in  43.87s; total  3053.85s\n",
      "\n",
      "Merging clusters\n",
      "44 units found, in  0.07s; total  3053.92s\n",
      "\n",
      "Saving to phy and computing refractory periods\n",
      "1 units found with good refractory periods\n",
      "\n",
      "Total runtime: 3056.19s = 00:50:56 h:m:s\n",
      "D:\\HexinData\\Dylan_2024-03-25_13-58-44_HPC\\data.bin\n",
      "False\n",
      "========================================\n",
      "Loading recording with SpikeInterface...\n",
      "number of samples: 209463996\n",
      "number of channels: 64\n",
      "numbef of segments: 3\n",
      "sampling rate: 30000.0\n",
      "dtype: int16\n",
      "========================================\n",
      "Converting 3490 data chunks with a chunksize of 60000 samples...\n",
      "25 of 3490 chunks converted...\n",
      "43 of 3490 chunks converted...\n",
      "50 of 3490 chunks converted...\n",
      "64 of 3490 chunks converted...\n",
      "75 of 3490 chunks converted...\n",
      "86 of 3490 chunks converted...\n",
      "99 of 3490 chunks converted...\n",
      "107 of 3490 chunks converted...\n",
      "121 of 3490 chunks converted...\n",
      "131 of 3490 chunks converted...\n",
      "144 of 3490 chunks converted...\n",
      "152 of 3490 chunks converted...\n",
      "167 of 3490 chunks converted...\n",
      "178 of 3490 chunks converted...\n",
      "193 of 3490 chunks converted...\n",
      "200 of 3490 chunks converted...\n",
      "210 of 3490 chunks converted...\n",
      "229 of 3490 chunks converted...\n",
      "236 of 3490 chunks converted...\n",
      "251 of 3490 chunks converted...\n",
      "260 of 3490 chunks converted...\n",
      "275 of 3490 chunks converted...\n",
      "287 of 3490 chunks converted...\n",
      "297 of 3490 chunks converted...\n",
      "312 of 3490 chunks converted...\n",
      "319 of 3490 chunks converted...\n",
      "332 of 3490 chunks converted...\n",
      "345 of 3490 chunks converted...\n",
      "354 of 3490 chunks converted...\n",
      "370 of 3490 chunks converted...\n",
      "385 of 3490 chunks converted...\n",
      "406 of 3490 chunks converted...\n",
      "419 of 3490 chunks converted...\n",
      "424 of 3490 chunks converted...\n",
      "444 of 3490 chunks converted...\n",
      "454 of 3490 chunks converted...\n",
      "471 of 3490 chunks converted...\n",
      "481 of 3490 chunks converted...\n",
      "498 of 3490 chunks converted...\n",
      "509 of 3490 chunks converted...\n",
      "521 of 3490 chunks converted...\n",
      "532 of 3490 chunks converted...\n",
      "548 of 3490 chunks converted...\n",
      "553 of 3490 chunks converted...\n",
      "569 of 3490 chunks converted...\n",
      "587 of 3490 chunks converted...\n",
      "598 of 3490 chunks converted...\n",
      "614 of 3490 chunks converted...\n",
      "627 of 3490 chunks converted...\n",
      "639 of 3490 chunks converted...\n",
      "657 of 3490 chunks converted...\n",
      "665 of 3490 chunks converted...\n",
      "683 of 3490 chunks converted...\n",
      "694 of 3490 chunks converted...\n",
      "709 of 3490 chunks converted...\n",
      "721 of 3490 chunks converted...\n",
      "736 of 3490 chunks converted...\n",
      "751 of 3490 chunks converted...\n",
      "763 of 3490 chunks converted...\n",
      "777 of 3490 chunks converted...\n",
      "796 of 3490 chunks converted...\n",
      "812 of 3490 chunks converted...\n",
      "824 of 3490 chunks converted...\n",
      "840 of 3490 chunks converted...\n",
      "854 of 3490 chunks converted...\n",
      "869 of 3490 chunks converted...\n",
      "885 of 3490 chunks converted...\n",
      "900 of 3490 chunks converted...\n",
      "913 of 3490 chunks converted...\n",
      "927 of 3490 chunks converted...\n",
      "944 of 3490 chunks converted...\n",
      "959 of 3490 chunks converted...\n",
      "974 of 3490 chunks converted...\n",
      "987 of 3490 chunks converted...\n",
      "1002 of 3490 chunks converted...\n",
      "1035 of 3490 chunks converted...\n",
      "1052 of 3490 chunks converted...\n",
      "1064 of 3490 chunks converted...\n",
      "1088 of 3490 chunks converted...\n",
      "1110 of 3490 chunks converted...\n",
      "1125 of 3490 chunks converted...\n",
      "1144 of 3490 chunks converted...\n",
      "1169 of 3490 chunks converted...\n",
      "1193 of 3490 chunks converted...\n",
      "1200 of 3490 chunks converted...\n",
      "1224 of 3490 chunks converted...\n",
      "1240 of 3490 chunks converted...\n",
      "1254 of 3490 chunks converted...\n",
      "1266 of 3490 chunks converted...\n",
      "1288 of 3490 chunks converted...\n",
      "1310 of 3490 chunks converted...\n",
      "1330 of 3490 chunks converted...\n",
      "1350 of 3490 chunks converted...\n",
      "1359 of 3490 chunks converted...\n",
      "1382 of 3490 chunks converted...\n",
      "1393 of 3490 chunks converted...\n",
      "1411 of 3490 chunks converted...\n",
      "1419 of 3490 chunks converted...\n",
      "1444 of 3490 chunks converted...\n",
      "1458 of 3490 chunks converted...\n",
      "1473 of 3490 chunks converted...\n",
      "1511 of 3490 chunks converted...\n",
      "1524 of 3490 chunks converted...\n",
      "1535 of 3490 chunks converted...\n",
      "1553 of 3490 chunks converted...\n",
      "1581 of 3490 chunks converted...\n",
      "1602 of 3490 chunks converted...\n",
      "1621 of 3490 chunks converted...\n",
      "1639 of 3490 chunks converted...\n",
      "1657 of 3490 chunks converted...\n",
      "1687 of 3490 chunks converted...\n",
      "1694 of 3490 chunks converted...\n",
      "1717 of 3490 chunks converted...\n",
      "1738 of 3490 chunks converted...\n",
      "1753 of 3490 chunks converted...\n",
      "1770 of 3490 chunks converted...\n",
      "1788 of 3490 chunks converted...\n",
      "1806 of 3490 chunks converted...\n",
      "1826 of 3490 chunks converted...\n",
      "1850 of 3490 chunks converted...\n",
      "1856 of 3490 chunks converted...\n",
      "1883 of 3490 chunks converted...\n",
      "1900 of 3490 chunks converted...\n",
      "1928 of 3490 chunks converted...\n",
      "1951 of 3490 chunks converted...\n",
      "1966 of 3490 chunks converted...\n",
      "1991 of 3490 chunks converted...\n",
      "2019 of 3490 chunks converted...\n",
      "2041 of 3490 chunks converted...\n",
      "2054 of 3490 chunks converted...\n",
      "2098 of 3490 chunks converted...\n",
      "2114 of 3490 chunks converted...\n",
      "2136 of 3490 chunks converted...\n",
      "2154 of 3490 chunks converted...\n",
      "2178 of 3490 chunks converted...\n",
      "2199 of 3490 chunks converted...\n",
      "2216 of 3490 chunks converted...\n",
      "2226 of 3490 chunks converted...\n",
      "2244 of 3490 chunks converted...\n",
      "2273 of 3490 chunks converted...\n",
      "2304 of 3490 chunks converted...\n",
      "2321 of 3490 chunks converted...\n",
      "2341 of 3490 chunks converted...\n",
      "2367 of 3490 chunks converted...\n",
      "2376 of 3490 chunks converted...\n",
      "2392 of 3490 chunks converted...\n",
      "2409 of 3490 chunks converted...\n",
      "2424 of 3490 chunks converted...\n",
      "2439 of 3490 chunks converted...\n",
      "2455 of 3490 chunks converted...\n",
      "2472 of 3490 chunks converted...\n",
      "2492 of 3490 chunks converted...\n",
      "2505 of 3490 chunks converted...\n",
      "2516 of 3490 chunks converted...\n",
      "2533 of 3490 chunks converted...\n",
      "2546 of 3490 chunks converted...\n",
      "2562 of 3490 chunks converted...\n",
      "2575 of 3490 chunks converted...\n",
      "2589 of 3490 chunks converted...\n",
      "2608 of 3490 chunks converted...\n",
      "2619 of 3490 chunks converted...\n",
      "2637 of 3490 chunks converted...\n",
      "2649 of 3490 chunks converted...\n",
      "2671 of 3490 chunks converted...\n",
      "2684 of 3490 chunks converted...\n",
      "2704 of 3490 chunks converted...\n",
      "2718 of 3490 chunks converted...\n",
      "2734 of 3490 chunks converted...\n",
      "2747 of 3490 chunks converted...\n",
      "2768 of 3490 chunks converted...\n",
      "2778 of 3490 chunks converted...\n",
      "2803 of 3490 chunks converted...\n",
      "2813 of 3490 chunks converted...\n",
      "2831 of 3490 chunks converted...\n",
      "2846 of 3490 chunks converted...\n",
      "2857 of 3490 chunks converted...\n",
      "2877 of 3490 chunks converted...\n",
      "2893 of 3490 chunks converted...\n",
      "2906 of 3490 chunks converted...\n",
      "2924 of 3490 chunks converted...\n",
      "2937 of 3490 chunks converted...\n",
      "2953 of 3490 chunks converted...\n",
      "2968 of 3490 chunks converted...\n",
      "2981 of 3490 chunks converted...\n",
      "3001 of 3490 chunks converted...\n",
      "3016 of 3490 chunks converted...\n",
      "3033 of 3490 chunks converted...\n",
      "3049 of 3490 chunks converted...\n",
      "3059 of 3490 chunks converted...\n",
      "3081 of 3490 chunks converted...\n",
      "3097 of 3490 chunks converted...\n",
      "3113 of 3490 chunks converted...\n",
      "3135 of 3490 chunks converted...\n",
      "3149 of 3490 chunks converted...\n",
      "3168 of 3490 chunks converted...\n",
      "3178 of 3490 chunks converted...\n",
      "3192 of 3490 chunks converted...\n",
      "3208 of 3490 chunks converted...\n",
      "3223 of 3490 chunks converted...\n",
      "3245 of 3490 chunks converted...\n",
      "3256 of 3490 chunks converted...\n",
      "3272 of 3490 chunks converted...\n",
      "3289 of 3490 chunks converted...\n",
      "3309 of 3490 chunks converted...\n",
      "3321 of 3490 chunks converted...\n",
      "3342 of 3490 chunks converted...\n",
      "3360 of 3490 chunks converted...\n",
      "3379 of 3490 chunks converted...\n",
      "3400 of 3490 chunks converted...\n",
      "3415 of 3490 chunks converted...\n",
      "3430 of 3490 chunks converted...\n",
      "3442 of 3490 chunks converted...\n",
      "3458 of 3490 chunks converted...\n",
      "3474 of 3490 chunks converted...\n",
      "3487 of 3490 chunks converted...\n",
      "3490 of 3490 chunks converted...\n",
      "Data conversion finished.\n",
      "========================================\n",
      "SpikeInterface recording contains no probe information,\n",
      "could not write .prb file.\n",
      "Using GPU for PyTorch computations. Specify `device` to change this.\n",
      "Preprocessing filters computed in  2.81s; total  2.81s\n",
      "\n",
      "computing drift\n",
      "Re-computing universal templates from data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3492/3492 [23:45<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drift computed in  1434.38s; total  1437.19s\n",
      "\n",
      "Extracting spikes using templates\n",
      "Re-computing universal templates from data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3492/3492 [23:37<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1419799 spikes extracted in  1421.47s; total  2858.69s\n",
      "\n",
      "First clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [01:27<00:00,  7.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 clusters found, in  87.89s; total  2946.58s\n",
      "\n",
      "Extracting spikes using cluster waveforms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3492/3492 [02:43<00:00, 21.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1268623 spikes extracted in  163.67s; total  3110.25s\n",
      "\n",
      "Final clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [01:25<00:00,  7.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 clusters found, in  85.74s; total  3195.99s\n",
      "\n",
      "Merging clusters\n",
      "38 units found, in  0.20s; total  3196.19s\n",
      "\n",
      "Saving to phy and computing refractory periods\n",
      "6 units found with good refractory periods\n",
      "\n",
      "Total runtime: 3209.69s = 00:53:30 h:m:s\n",
      "D:\\HexinData\\Dylan_2024-03-26_14-09-51_HPC\\data.bin\n",
      "False\n",
      "========================================\n",
      "Loading recording with SpikeInterface...\n",
      "number of samples: 210989776\n",
      "number of channels: 64\n",
      "numbef of segments: 2\n",
      "sampling rate: 30000.0\n",
      "dtype: int16\n",
      "========================================\n",
      "Converting 3516 data chunks with a chunksize of 60000 samples...\n",
      "24 of 3516 chunks converted...\n",
      "41 of 3516 chunks converted...\n",
      "58 of 3516 chunks converted...\n",
      "71 of 3516 chunks converted...\n",
      "83 of 3516 chunks converted...\n",
      "98 of 3516 chunks converted...\n",
      "111 of 3516 chunks converted...\n",
      "126 of 3516 chunks converted...\n",
      "140 of 3516 chunks converted...\n",
      "154 of 3516 chunks converted...\n",
      "164 of 3516 chunks converted...\n",
      "182 of 3516 chunks converted...\n",
      "196 of 3516 chunks converted...\n",
      "213 of 3516 chunks converted...\n",
      "223 of 3516 chunks converted...\n",
      "237 of 3516 chunks converted...\n",
      "249 of 3516 chunks converted...\n",
      "269 of 3516 chunks converted...\n",
      "280 of 3516 chunks converted...\n",
      "297 of 3516 chunks converted...\n",
      "307 of 3516 chunks converted...\n",
      "328 of 3516 chunks converted...\n",
      "336 of 3516 chunks converted...\n",
      "358 of 3516 chunks converted...\n",
      "376 of 3516 chunks converted...\n",
      "383 of 3516 chunks converted...\n",
      "404 of 3516 chunks converted...\n",
      "409 of 3516 chunks converted...\n",
      "428 of 3516 chunks converted...\n",
      "438 of 3516 chunks converted...\n",
      "452 of 3516 chunks converted...\n",
      "462 of 3516 chunks converted...\n",
      "479 of 3516 chunks converted...\n",
      "485 of 3516 chunks converted...\n",
      "505 of 3516 chunks converted...\n",
      "509 of 3516 chunks converted...\n",
      "528 of 3516 chunks converted...\n",
      "544 of 3516 chunks converted...\n",
      "553 of 3516 chunks converted...\n",
      "564 of 3516 chunks converted...\n",
      "577 of 3516 chunks converted...\n",
      "592 of 3516 chunks converted...\n",
      "605 of 3516 chunks converted...\n",
      "617 of 3516 chunks converted...\n",
      "632 of 3516 chunks converted...\n",
      "646 of 3516 chunks converted...\n",
      "657 of 3516 chunks converted...\n",
      "667 of 3516 chunks converted...\n",
      "683 of 3516 chunks converted...\n",
      "695 of 3516 chunks converted...\n",
      "706 of 3516 chunks converted...\n",
      "725 of 3516 chunks converted...\n",
      "732 of 3516 chunks converted...\n",
      "744 of 3516 chunks converted...\n",
      "754 of 3516 chunks converted...\n",
      "762 of 3516 chunks converted...\n",
      "778 of 3516 chunks converted...\n",
      "785 of 3516 chunks converted...\n",
      "798 of 3516 chunks converted...\n",
      "806 of 3516 chunks converted...\n",
      "819 of 3516 chunks converted...\n",
      "826 of 3516 chunks converted...\n",
      "833 of 3516 chunks converted...\n",
      "845 of 3516 chunks converted...\n",
      "853 of 3516 chunks converted...\n",
      "864 of 3516 chunks converted...\n",
      "874 of 3516 chunks converted...\n",
      "888 of 3516 chunks converted...\n",
      "895 of 3516 chunks converted...\n",
      "906 of 3516 chunks converted...\n",
      "917 of 3516 chunks converted...\n",
      "925 of 3516 chunks converted...\n",
      "942 of 3516 chunks converted...\n",
      "947 of 3516 chunks converted...\n",
      "965 of 3516 chunks converted...\n",
      "974 of 3516 chunks converted...\n",
      "987 of 3516 chunks converted...\n",
      "999 of 3516 chunks converted...\n",
      "1010 of 3516 chunks converted...\n",
      "1020 of 3516 chunks converted...\n",
      "1032 of 3516 chunks converted...\n",
      "1043 of 3516 chunks converted...\n",
      "1052 of 3516 chunks converted...\n",
      "1067 of 3516 chunks converted...\n",
      "1074 of 3516 chunks converted...\n",
      "1086 of 3516 chunks converted...\n",
      "1096 of 3516 chunks converted...\n",
      "1111 of 3516 chunks converted...\n",
      "1120 of 3516 chunks converted...\n",
      "1131 of 3516 chunks converted...\n",
      "1147 of 3516 chunks converted...\n",
      "1154 of 3516 chunks converted...\n",
      "1167 of 3516 chunks converted...\n",
      "1176 of 3516 chunks converted...\n",
      "1185 of 3516 chunks converted...\n",
      "1200 of 3516 chunks converted...\n",
      "1207 of 3516 chunks converted...\n",
      "1222 of 3516 chunks converted...\n",
      "1228 of 3516 chunks converted...\n",
      "1246 of 3516 chunks converted...\n",
      "1253 of 3516 chunks converted...\n",
      "1267 of 3516 chunks converted...\n",
      "1274 of 3516 chunks converted...\n",
      "1282 of 3516 chunks converted...\n",
      "1294 of 3516 chunks converted...\n",
      "1302 of 3516 chunks converted...\n",
      "1319 of 3516 chunks converted...\n",
      "1327 of 3516 chunks converted...\n",
      "1334 of 3516 chunks converted...\n",
      "1348 of 3516 chunks converted...\n",
      "1353 of 3516 chunks converted...\n",
      "1369 of 3516 chunks converted...\n",
      "1379 of 3516 chunks converted...\n",
      "1394 of 3516 chunks converted...\n",
      "1407 of 3516 chunks converted...\n",
      "1415 of 3516 chunks converted...\n",
      "1424 of 3516 chunks converted...\n",
      "1441 of 3516 chunks converted...\n",
      "1449 of 3516 chunks converted...\n",
      "1467 of 3516 chunks converted...\n",
      "1473 of 3516 chunks converted...\n",
      "1487 of 3516 chunks converted...\n",
      "1497 of 3516 chunks converted...\n",
      "1514 of 3516 chunks converted...\n",
      "1520 of 3516 chunks converted...\n",
      "1534 of 3516 chunks converted...\n",
      "1543 of 3516 chunks converted...\n",
      "1554 of 3516 chunks converted...\n",
      "1564 of 3516 chunks converted...\n",
      "1574 of 3516 chunks converted...\n",
      "1586 of 3516 chunks converted...\n",
      "1596 of 3516 chunks converted...\n",
      "1611 of 3516 chunks converted...\n",
      "1621 of 3516 chunks converted...\n",
      "1626 of 3516 chunks converted...\n",
      "1645 of 3516 chunks converted...\n",
      "1667 of 3516 chunks converted...\n",
      "1675 of 3516 chunks converted...\n",
      "1688 of 3516 chunks converted...\n",
      "1697 of 3516 chunks converted...\n",
      "1707 of 3516 chunks converted...\n",
      "1719 of 3516 chunks converted...\n",
      "1729 of 3516 chunks converted...\n",
      "1745 of 3516 chunks converted...\n",
      "1753 of 3516 chunks converted...\n",
      "1765 of 3516 chunks converted...\n",
      "1772 of 3516 chunks converted...\n",
      "1786 of 3516 chunks converted...\n",
      "1796 of 3516 chunks converted...\n",
      "1808 of 3516 chunks converted...\n",
      "1818 of 3516 chunks converted...\n",
      "1826 of 3516 chunks converted...\n",
      "1842 of 3516 chunks converted...\n",
      "1850 of 3516 chunks converted...\n",
      "1867 of 3516 chunks converted...\n",
      "1874 of 3516 chunks converted...\n",
      "1881 of 3516 chunks converted...\n",
      "1898 of 3516 chunks converted...\n",
      "1904 of 3516 chunks converted...\n",
      "1920 of 3516 chunks converted...\n",
      "1926 of 3516 chunks converted...\n",
      "1941 of 3516 chunks converted...\n",
      "1949 of 3516 chunks converted...\n",
      "1958 of 3516 chunks converted...\n",
      "1974 of 3516 chunks converted...\n",
      "1986 of 3516 chunks converted...\n",
      "1995 of 3516 chunks converted...\n",
      "2006 of 3516 chunks converted...\n",
      "2020 of 3516 chunks converted...\n",
      "2029 of 3516 chunks converted...\n",
      "2040 of 3516 chunks converted...\n",
      "2050 of 3516 chunks converted...\n",
      "2063 of 3516 chunks converted...\n",
      "2076 of 3516 chunks converted...\n",
      "2083 of 3516 chunks converted...\n",
      "2095 of 3516 chunks converted...\n",
      "2105 of 3516 chunks converted...\n",
      "2116 of 3516 chunks converted...\n",
      "2127 of 3516 chunks converted...\n",
      "2137 of 3516 chunks converted...\n",
      "2146 of 3516 chunks converted...\n",
      "2163 of 3516 chunks converted...\n",
      "2172 of 3516 chunks converted...\n",
      "2181 of 3516 chunks converted...\n",
      "2198 of 3516 chunks converted...\n",
      "2210 of 3516 chunks converted...\n",
      "2223 of 3516 chunks converted...\n",
      "2235 of 3516 chunks converted...\n",
      "2248 of 3516 chunks converted...\n",
      "2259 of 3516 chunks converted...\n",
      "2268 of 3516 chunks converted...\n",
      "2283 of 3516 chunks converted...\n",
      "2291 of 3516 chunks converted...\n",
      "2306 of 3516 chunks converted...\n",
      "2312 of 3516 chunks converted...\n",
      "2330 of 3516 chunks converted...\n",
      "2336 of 3516 chunks converted...\n",
      "2347 of 3516 chunks converted...\n",
      "2360 of 3516 chunks converted...\n",
      "2369 of 3516 chunks converted...\n",
      "2379 of 3516 chunks converted...\n",
      "2394 of 3516 chunks converted...\n",
      "2405 of 3516 chunks converted...\n",
      "2417 of 3516 chunks converted...\n",
      "2422 of 3516 chunks converted...\n",
      "2435 of 3516 chunks converted...\n",
      "2447 of 3516 chunks converted...\n",
      "2460 of 3516 chunks converted...\n",
      "2469 of 3516 chunks converted...\n",
      "2481 of 3516 chunks converted...\n",
      "2491 of 3516 chunks converted...\n",
      "2502 of 3516 chunks converted...\n",
      "2514 of 3516 chunks converted...\n",
      "2522 of 3516 chunks converted...\n",
      "2536 of 3516 chunks converted...\n",
      "2544 of 3516 chunks converted...\n",
      "2554 of 3516 chunks converted...\n",
      "2567 of 3516 chunks converted...\n",
      "2576 of 3516 chunks converted...\n",
      "2588 of 3516 chunks converted...\n",
      "2602 of 3516 chunks converted...\n",
      "2608 of 3516 chunks converted...\n",
      "2625 of 3516 chunks converted...\n",
      "2630 of 3516 chunks converted...\n",
      "2649 of 3516 chunks converted...\n",
      "2655 of 3516 chunks converted...\n",
      "2667 of 3516 chunks converted...\n",
      "2681 of 3516 chunks converted...\n",
      "2698 of 3516 chunks converted...\n",
      "2705 of 3516 chunks converted...\n",
      "2721 of 3516 chunks converted...\n",
      "2727 of 3516 chunks converted...\n",
      "2747 of 3516 chunks converted...\n",
      "2752 of 3516 chunks converted...\n",
      "2760 of 3516 chunks converted...\n",
      "2777 of 3516 chunks converted...\n",
      "2783 of 3516 chunks converted...\n",
      "2802 of 3516 chunks converted...\n",
      "2806 of 3516 chunks converted...\n",
      "2820 of 3516 chunks converted...\n",
      "2832 of 3516 chunks converted...\n",
      "2846 of 3516 chunks converted...\n",
      "2853 of 3516 chunks converted...\n",
      "2863 of 3516 chunks converted...\n",
      "2877 of 3516 chunks converted...\n",
      "2886 of 3516 chunks converted...\n",
      "2895 of 3516 chunks converted...\n",
      "2909 of 3516 chunks converted...\n",
      "2917 of 3516 chunks converted...\n",
      "2929 of 3516 chunks converted...\n",
      "2937 of 3516 chunks converted...\n",
      "2949 of 3516 chunks converted...\n",
      "2961 of 3516 chunks converted...\n",
      "2964 of 3516 chunks converted...\n",
      "2979 of 3516 chunks converted...\n",
      "2987 of 3516 chunks converted...\n",
      "2997 of 3516 chunks converted...\n",
      "3008 of 3516 chunks converted...\n",
      "3016 of 3516 chunks converted...\n",
      "3032 of 3516 chunks converted...\n",
      "3036 of 3516 chunks converted...\n",
      "3056 of 3516 chunks converted...\n",
      "3064 of 3516 chunks converted...\n",
      "3073 of 3516 chunks converted...\n",
      "3086 of 3516 chunks converted...\n",
      "3094 of 3516 chunks converted...\n",
      "3108 of 3516 chunks converted...\n",
      "3115 of 3516 chunks converted...\n",
      "3127 of 3516 chunks converted...\n",
      "3136 of 3516 chunks converted...\n",
      "3147 of 3516 chunks converted...\n",
      "3162 of 3516 chunks converted...\n",
      "3169 of 3516 chunks converted...\n",
      "3179 of 3516 chunks converted...\n",
      "3191 of 3516 chunks converted...\n",
      "3200 of 3516 chunks converted...\n",
      "3213 of 3516 chunks converted...\n",
      "3225 of 3516 chunks converted...\n",
      "3233 of 3516 chunks converted...\n",
      "3248 of 3516 chunks converted...\n",
      "3255 of 3516 chunks converted...\n",
      "3268 of 3516 chunks converted...\n",
      "3280 of 3516 chunks converted...\n",
      "3286 of 3516 chunks converted...\n",
      "3301 of 3516 chunks converted...\n",
      "3308 of 3516 chunks converted...\n",
      "3319 of 3516 chunks converted...\n",
      "3337 of 3516 chunks converted...\n",
      "3345 of 3516 chunks converted...\n",
      "3357 of 3516 chunks converted...\n",
      "3368 of 3516 chunks converted...\n",
      "3377 of 3516 chunks converted...\n",
      "3392 of 3516 chunks converted...\n",
      "3401 of 3516 chunks converted...\n",
      "3412 of 3516 chunks converted...\n",
      "3422 of 3516 chunks converted...\n",
      "3435 of 3516 chunks converted...\n",
      "3443 of 3516 chunks converted...\n",
      "3455 of 3516 chunks converted...\n",
      "3469 of 3516 chunks converted...\n",
      "3477 of 3516 chunks converted...\n",
      "3492 of 3516 chunks converted...\n",
      "3502 of 3516 chunks converted...\n",
      "3516 of 3516 chunks converted...\n",
      "Data conversion finished.\n",
      "========================================\n",
      "SpikeInterface recording contains no probe information,\n",
      "could not write .prb file.\n",
      "Using GPU for PyTorch computations. Specify `device` to change this.\n",
      "Preprocessing filters computed in  2.72s; total  2.72s\n",
      "\n",
      "computing drift\n",
      "Re-computing universal templates from data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3517/3517 [24:03<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drift computed in  1452.19s; total  1454.91s\n",
      "\n",
      "Extracting spikes using templates\n",
      "Re-computing universal templates from data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3517/3517 [24:13<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1470905 spikes extracted in  1457.71s; total  2922.87s\n",
      "\n",
      "First clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [01:30<00:00,  8.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 clusters found, in  90.24s; total  3013.11s\n",
      "\n",
      "Extracting spikes using cluster waveforms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3517/3517 [02:56<00:00, 19.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286544 spikes extracted in  176.40s; total  3189.51s\n",
      "\n",
      "Final clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [01:16<00:00,  6.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 clusters found, in  76.08s; total  3265.59s\n",
      "\n",
      "Merging clusters\n",
      "31 units found, in  0.23s; total  3265.82s\n",
      "\n",
      "Saving to phy and computing refractory periods\n",
      "12 units found with good refractory periods\n",
      "\n",
      "Total runtime: 3278.96s = 00:54:39 h:m:s\n"
     ]
    }
   ],
   "source": [
    "mypath = Path('D:\\HexinData')\n",
    "for ii,f in enumerate(listdir(mypath)):\n",
    "    run_ks_files(mypath / f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91b81a91-3219-4a67-a46a-a99216153d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_timing_ephys(base_dir,ttl_dir):    \n",
    "    timefile = ttl_dir / 'timestamps.npy'\n",
    "    statefile = ttl_dir / 'states.npy'\n",
    "    outfile = base_dir / 'timestamps.data'\n",
    "    \n",
    "    a = np.load(timefile)\n",
    "    b = np.load(statefile)\n",
    "    \n",
    "    c = a[b==1]\n",
    "    c.shape = (c.size,1)\n",
    "    \n",
    "    d = (a[b==-1]-a[b==1])*1000\n",
    "    d.shape = (d.size,1)\n",
    "    \n",
    "    np.savetxt(outfile,np.hstack([c,d]),fmt='%.6f,%.1f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5de29f3e-50c8-4084-b88a-61bd9dbbd6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = mypath / f/ 'Record Node 113/experiment1/recording1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c34bdbb3-68f2-43ab-941a-7bdb511f9776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('D:/HexinData/Dylan_2024-03-26_14-09-51_HPC/Record Node 113/experiment1/recording1')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a3d569-90fa-40e0-9368-74f60c507a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = mypath / f/ 'Record Node 113/experiment1/recording1'\n",
    "header_file = base_folder / 'structure.oebin'\n",
    "session_file = list(base_folder.glob(\"*.sqlite\"))[0]\n",
    "timestamp_file = base_folder / 'timestamps.data'\n",
    "alignment_file = base_folder / 'alignmentinfo_README.txt'\n",
    "\n",
    "with open(header_file) as file_:\n",
    "    header_data = json.load(file_)\n",
    "    \n",
    "data_path = base_folder / 'continuous' / header_data['continuous'][0]['folder_name'] / 'continuous.dat'\n",
    "ttl_path = base_folder / 'events' / header_data['events'][0]['folder_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3137adf5-30e6-4963-9221-96e2c32b5d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract timing signal if not done already\n",
    "if not os.path.exists(timestamp_file):\n",
    "    extract_timing_ephys(base_folder,ttl_path)\n",
    "    # For neuropixels recordings, run the following line instead:\n",
    "    # subprocess.run(['python','D:/SpikeInterface/extract_timing.py',data_path,str(sample_rate),str(nchannels),str(nchannels-1),\"2\"])\n",
    "\n",
    "# Align timing if not done already\n",
    "if not os.path.exists(alignment_file):\n",
    "    subprocess.run(['D:/SpikeInterface/align_timestamps.exe',session_file,timestamp_file,alignment_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fa9bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec, rcParams\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['axes.spines.right'] = False\n",
    "gray = .5 * np.ones(3)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10), dpi=100)\n",
    "grid = gridspec.GridSpec(3, 3, figure=fig, hspace=0.5, wspace=0.5)\n",
    "\n",
    "ax = fig.add_subplot(grid[0,0])\n",
    "ax.plot(np.arange(0, ops['Nbatches'])*2, dshift);\n",
    "ax.set_xlabel('time (sec.)')\n",
    "ax.set_ylabel('drift (um)')\n",
    "\n",
    "ax = fig.add_subplot(grid[0,1:])\n",
    "t0 = 0\n",
    "t1 = np.nonzero(st > ops['fs']*5)[0][0]\n",
    "ax.scatter(st[t0:t1]/30000., chan_best[clu[t0:t1]], s=0.5, color='k', alpha=0.25)\n",
    "ax.set_xlim([0, 5])\n",
    "ax.set_ylim([chan_map.max(), 0])\n",
    "ax.set_xlabel('time (sec.)')\n",
    "ax.set_ylabel('channel')\n",
    "ax.set_title('spikes from units')\n",
    "\n",
    "ax = fig.add_subplot(grid[1,0])\n",
    "nb=ax.hist(firing_rates, 20, color=gray)\n",
    "ax.set_xlabel('firing rate (Hz)')\n",
    "ax.set_ylabel('# of units')\n",
    "\n",
    "ax = fig.add_subplot(grid[1,1])\n",
    "nb=ax.hist(camps, 20, color=gray)\n",
    "ax.set_xlabel('amplitude')\n",
    "ax.set_ylabel('# of units')\n",
    "\n",
    "ax = fig.add_subplot(grid[1,2])\n",
    "nb=ax.hist(np.minimum(100, contam_pct), np.arange(0,105,5), color=gray)\n",
    "ax.plot([10, 10], [0, nb[0].max()], 'k--')\n",
    "ax.set_xlabel('% contamination')\n",
    "ax.set_ylabel('# of units')\n",
    "ax.set_title('< 10% = good units')\n",
    "\n",
    "for k in range(2):\n",
    "    ax = fig.add_subplot(grid[2,k])\n",
    "    is_ref = contam_pct<10.\n",
    "    ax.scatter(firing_rates[~is_ref], camps[~is_ref], s=3, color='r', label='mua', alpha=0.25)\n",
    "    ax.scatter(firing_rates[is_ref], camps[is_ref], s=3, color='b', label='good', alpha=0.25)\n",
    "    ax.set_ylabel('amplitude (a.u.)')\n",
    "    ax.set_xlabel('firing rate (Hz)')\n",
    "    ax.legend()\n",
    "    if k==1:\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_title('loglog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b792a2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = ops['probe']\n",
    "# x and y position of probe sites\n",
    "xc, yc = probe['xc'], probe['yc']\n",
    "nc = 16 # number of channels to show\n",
    "good_units = np.nonzero(contam_pct <= 0.1)[0]\n",
    "mua_units = np.nonzero(contam_pct > 0.1)[0]\n",
    "\n",
    "\n",
    "gstr = ['good', 'mua']\n",
    "for j in range(2):\n",
    "    print(f'~~~~~~~~~~~~~~ {gstr[j]} units ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print('title = number of spikes from each unit')\n",
    "    units = good_units if j==0 else mua_units\n",
    "    fig = plt.figure(figsize=(12,3), dpi=150)\n",
    "    grid = gridspec.GridSpec(2,20, figure=fig, hspace=0.25, wspace=0.5)\n",
    "\n",
    "    for k in range(40):\n",
    "        wi = units[np.random.randint(len(units))]\n",
    "        wv = templates[wi].copy()\n",
    "        cb = chan_best[wi]\n",
    "        nsp = (clu==wi).sum()\n",
    "\n",
    "        ax = fig.add_subplot(grid[k//20, k%20])\n",
    "        n_chan = wv.shape[-1]\n",
    "        ic0 = max(0, cb-nc//2)\n",
    "        ic1 = min(n_chan, cb+nc//2)\n",
    "        wv = wv[:, ic0:ic1]\n",
    "        x0, y0 = xc[ic0:ic1], yc[ic0:ic1]\n",
    "\n",
    "        amp = 4\n",
    "        for ii, (xi,yi) in enumerate(zip(x0,y0)):\n",
    "            t = np.arange(-wv.shape[0]//2,wv.shape[0]//2,1,'float32')\n",
    "            t /= wv.shape[0] / 20\n",
    "            ax.plot(xi + t, yi + wv[:,ii]*amp, lw=0.5, color='k')\n",
    "\n",
    "        ax.set_title(f'{nsp}', fontsize='small')\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3a16a65-d438-4537-b890-f818c567626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = Path(\"D:/HexinData/Dylan_2024-03-19_14-42-22_HPC/Record Node 113/experiment1/recording1\")\n",
    "header_file = base_folder / 'structure.oebin'\n",
    "session_file = list(base_folder.glob(\"*.sqlite\"))[0]\n",
    "timestamp_file = base_folder / 'timestamps.data'\n",
    "alignment_file = base_folder / 'alignmentinfo_README.txt'\n",
    "\n",
    "with open(header_file) as file_:\n",
    "    header_data = json.load(file_)\n",
    "    \n",
    "data_path = base_folder / 'continuous' / header_data['continuous'][0]['folder_name'] / 'continuous.dat'\n",
    "ttl_path = base_folder / 'events' / header_data['events'][0]['folder_name']\n",
    "nchannels = header_data['continuous'][0]['num_channels']\n",
    "sample_rate = header_data['continuous'][0]['sample_rate']\n",
    "chan_ids = [0]*nchannels\n",
    "\n",
    "# This assumes the open ephys header format where channels are named \"CH1\", \"CH2\", ... , \"CHn\"\n",
    "for index, channel in enumerate(header_data['continuous'][0]['channels']):\n",
    "    chan_ids[index] = int(channel['channel_name'][2:])-1\n",
    "\n",
    "sorting_alg = 'kilosort2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ac528d5d-6d87-4db6-a51b-558bc6991b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_raw = si.read_binary(file_paths=data_path,sampling_frequency=sample_rate,dtype='int16',num_chan=nchannels)\n",
    "recording_raw.annotate(is_filtered=False)\n",
    "channel_ids = recording_raw.get_channel_ids()\n",
    "fs = recording_raw.get_sampling_frequency()\n",
    "num_chan = recording_raw.get_num_channels()\n",
    "num_segments = recording_raw.get_num_segments()\n",
    "\n",
    "print(f'Channel ids: {channel_ids}')\n",
    "print(f'Sampling frequency: {fs}')\n",
    "print(f'Number of channels: {num_chan}')\n",
    "print(f\"Number of segments: {num_segments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2d3600df-a4a1-45fa-b7b1-80bf9808976b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BandpassFilterRecording: 64 channels - 1 segments - 30.0kHz - 7402.021s"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = recording_f.get_sampling_frequency()\n",
    "\n",
    "#recording_sub = recording_cmr.frame_slice(start_frame=0*fs, end_frame=1500*fs)\n",
    "recording_sub = recording_f\n",
    "recording_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ce1035b3-e18c-4748-9b45-9ff6940e3d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_kwargs = dict(n_jobs=10, chunk_duration=\"1s\", progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c4b641b0-c096-4486-b224-9df8ffb79660",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dir = base_folder / \"preprocessed\"\n",
    "\n",
    "if preprocessed_dir.is_dir():\n",
    "    recording_saved = si.load_extractor(preprocessed_dir)\n",
    "else:\n",
    "    recording_saved = recording_sub.save(folder=preprocessed_dir, **job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4c433bb-c9c7-4f91-8142-98748e6174c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting KILOSORT3_PATH environment variable for subprocess calls to: D:\\Kilosort-main\\Kilosort-main\n",
      "Setting KILOSORT2_5_PATH environment variable for subprocess calls to: D:\\Kilosort-2.5\n",
      "Setting KILOSORT2_PATH environment variable for subprocess calls to: D:\\Kilosort-2.0\\Kilosort-2.0\n"
     ]
    }
   ],
   "source": [
    "ss.Kilosort3Sorter.set_kilosort3_path('D:\\Kilosort-main\\Kilosort-main')\n",
    "ss.Kilosort2_5Sorter.set_kilosort2_5_path('D:\\Kilosort-2.5')\n",
    "ss.Kilosort2Sorter.set_kilosort2_path('D:\\Kilosort-2.0\\Kilosort-2.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e9226112-6741-4a3a-9b00-a234150f56e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING SHELL SCRIPT: C:\\Users\\leelab\\AppData\\Local\\Temp\\tmp_shellscripto7o6hnhi\\script.bat\n",
      "RUNNING SHELL SCRIPT: C:\\Users\\leelab\\AppData\\Local\\Temp\\tmp_shellscript3z5rolth\\script.bat\n",
      "RUNNING SHELL SCRIPT: C:\\Users\\leelab\\AppData\\Local\\Temp\\tmp_shellscriptf21wjme8\\script.bat\n",
      "RUNNING SHELL SCRIPT: D:\\HexinData\\Dylan_2024-03-19_14-42-22_HPC\\Record Node 113\\experiment1\\recording1\\output\\kilosort3\\run_kilosort3.bat\n",
      "\n",
      "\n",
      "C:\\Users\\leelab\\Documents\\GitHub\\MazeTaskAnalysis>D:\n",
      "\n",
      "\n",
      "\n",
      "D:\\>cd D:\\HexinData\\Dylan_2024-03-19_14-42-22_HPC\\Record Node 113\\experiment1\\recording1\\output\\kilosort3 \n",
      "\n",
      "\n",
      "\n",
      "D:\\HexinData\\Dylan_2024-03-19_14-42-22_HPC\\Record Node 113\\experiment1\\recording1\\output\\kilosort3>matlab -nosplash -wait -r \"kilosort3_master('D:\\HexinData\\Dylan_2024-03-19_14-42-22_HPC\\Record Node 113\\experiment1\\recording1\\output\\kilosort3', 'D:\\Kilosort-main\\Kilosort-main')\" \n",
      "\n",
      "kilosort3 run time 1064.03s\n"
     ]
    }
   ],
   "source": [
    "# run spike sorting on entire recording\n",
    "output_folder = base_folder / 'output' / sorting_alg\n",
    "    \n",
    "sorting_output = ss.run_sorter(sorting_alg, recording_saved,\n",
    "                             output_folder=output_folder,\n",
    "                             verbose=True, **job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c37d1fb-8fe9-454c-8470-03bfba971c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_timing_ephys(base_dir,ttl_dir):    \n",
    "    timefile = ttl_dir / 'timestamps.npy'\n",
    "    statefile = ttl_dir / 'states.npy'\n",
    "    outfile = base_dir / 'timestamps.data'\n",
    "    \n",
    "    a = np.load(timefile)\n",
    "    b = np.load(statefile)\n",
    "    \n",
    "    c = a[b==1]\n",
    "    c.shape = (c.size,1)\n",
    "    \n",
    "    d = (a[b==-1]-a[b==1])*1000\n",
    "    d.shape = (d.size,1)\n",
    "    \n",
    "    np.savetxt(outfile,np.hstack([c,d]),fmt='%.6f,%.1f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f8d903f1-6296-4c1f-8251-f26203478e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sorting_alg == 'kilosort2':\n",
    "    # Update params.py to point to temp_wh.dat locally\n",
    "    params_file = output_folder / 'params.py'\n",
    "    \n",
    "    with open(params_file,'r') as file_:\n",
    "        params_ = file_.readlines()\n",
    "    \n",
    "    if params_:\n",
    "        params_[0] = 'dat_path = \\'temp_wh.dat\\'\\n'\n",
    "        with open(params_file,'w') as file_:\n",
    "            file_.writelines(params_)\n",
    "\n",
    "    # Delete the unnecessary recording.dat if it exists\n",
    "    tbd = output_folder / 'recording.dat'\n",
    "    if os.path.exists(tbd):\n",
    "        os.remove(tbd)\n",
    "\n",
    "    # Extract phy waveforms\n",
    "    phy_script = r'D:\\SpikeInterface\\run_phy.bat'\n",
    "    subprocess.run([phy_script, output_folder])\n",
    "\n",
    "    # Extract timing signal if not done already\n",
    "    if not os.path.exists(timestamp_file):\n",
    "        extract_timing_ephys(base_folder,ttl_path)\n",
    "        # For neuropixels recordings, run the following line instead:\n",
    "        # subprocess.run(['python','D:/SpikeInterface/extract_timing.py',data_path,str(sample_rate),str(nchannels),str(nchannels-1),\"2\"])\n",
    "\n",
    "    # Align timing if not done already\n",
    "    if not os.path.exists(alignment_file):\n",
    "        subprocess.run(['D:/SpikeInterface/align_timestamps.exe',session_file,timestamp_file,alignment_file])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "742194c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.fromfile(output_folder /'temp_wh.dat', dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fa2fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes = np.load(output_folder / 'spike_times.npy')\n",
    "clusters = np.load(output_folder / 'spike_clusters.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeed2258",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = base_folder / 'output' / sorting_alg\n",
    "\n",
    "model = load_model(output_folder / 'params.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13370dfb-83d3-449d-8b60-2752147a3429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Run Phy:\n",
    "# Activate conda environment: 'conda activate phy2'\n",
    "# Run phy: 'phy template-gui params.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9483de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = sqlite3.connect('D:/HexinData/20240226_Dylan_Day1/Session_2024_01_03__13_22_47.sqlite')\n",
    "# query = \"SELECT aligncode,timestamp FROM behavioralalignevents ORDER BY aligneventnumber\"\n",
    "# cursor = db.cursor()\n",
    "# a = np.array(cursor.execute(query).fetchall())\n",
    "# b = np.array(pd.read_csv('D:/Recordings/2024-01-05_11-30-26/Record Node 113/experiment1/recording1/timestamps.data',header=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d0815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session = Session('D:\\HexinData\\2024-02-26_14-01-21')\n",
    "# recording = session.recordnodes[0].recordings[0]\n",
    "# samplesize = recording.continuous[0].samples.shape[0]\n",
    "# chunksize=30000*60  ## 1 minute\n",
    "# bins = int(np.floor(samplesize/chunksize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8b18c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allspikes=[]\n",
    "\n",
    "# for bb in range(bins):\n",
    "    \n",
    "#     voltage = recording.continuous[0].get_samples(start_sample_index=chunksize*bb, end_sample_index=chunksize*(bb+1))\n",
    "#     ts = recording.continuous[0].timestamps[chunksize*bb:chunksize*(bb+1)]\n",
    "#     x=voltage[:,16]\n",
    "#     sos = signal.butter(10, 1000, btype='highpass',output='sos',fs=30000)\n",
    "#     filtered = signal.sosfilt(sos, x)\n",
    "#     spikes, _ = find_peaks(filtered, height=75,distance=100)\n",
    "#     Allspikes=np.append(Allspikes,ts[spikes])\n",
    "    \n",
    "# Spikes_alinged=Allspikes*1.0000028-183.3522801"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
